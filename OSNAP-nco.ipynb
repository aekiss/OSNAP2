{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f9a4d046-b316-44df-9237-22f0a5a972e9",
   "metadata": {},
   "source": [
    "# OSNAP data extraction using NCO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0efb3b89-3d40-4a98-afd5-cf45848ef21e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import cosima_cookbook as cc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import flox  # for faster groupby in xarray with dask\n",
    "from dask.distributed import Client\n",
    "from datetime import timedelta, date\n",
    "import calendar\n",
    "import os\n",
    "import shutil\n",
    "import glob\n",
    "from collections import OrderedDict\n",
    "import subprocess\n",
    "import warnings\n",
    "import logging\n",
    "logging.captureWarnings(True)\n",
    "logging.getLogger('py.warnings').setLevel(logging.ERROR)\n",
    "logging.getLogger('distributed.utils_perf').setLevel(logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c627f3e1-69b5-4912-b979-a29b9282cc02",
   "metadata": {},
   "outputs": [],
   "source": [
    "import climtas.nci\n",
    "climtas.nci.GadiClient(malloc_trim_threshold='64kib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42c1c643-7989-48d3-a7e8-0b7391ca770c",
   "metadata": {},
   "outputs": [],
   "source": [
    "session = cc.database.create_session()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fb0528b-9774-4296-898e-b8558441022c",
   "metadata": {},
   "source": [
    "## Initialise data structure and define helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce5a7fa4-16cb-4d14-848d-c51db44fad0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# WARNING! FORGETS ALL LOADED DATA!\n",
    "data = OrderedDict() # init nested dict of experiments and their analyses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "556400c1-64f5-49b2-b819-34219852aa4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def addexpt(k, d):\n",
    "    if k in data:\n",
    "        print('skipped {}: already exists'.format(k))\n",
    "    else:\n",
    "        data[k] = d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50e2226a-d4ee-4ef4-aba0-07171cc7cbc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dictget(d, l):\n",
    "    \"\"\"\n",
    "    Get item in nested dict using a list of keys\n",
    "\n",
    "    d: nested dict\n",
    "    l: list of keys\n",
    "    \"\"\"\n",
    "    if len(l) == 1:\n",
    "        return d[l[0]]\n",
    "    return dictget(d[l[0]], l[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a41f9d20-aad9-4fa8-9c43-043099f82f35",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dictknown(d, l):\n",
    "    \"\"\"\n",
    "    Return true if list of keys is valid in nested dict\n",
    "\n",
    "    d: nested dict\n",
    "    l: list of keys\n",
    "    \"\"\"    \n",
    "    while len(l)>0 and l[0] in d:\n",
    "        d = d[l[0]]\n",
    "        l = l[1:]\n",
    "    return len(l) == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a05eb897-90af-43d0-b91d-e3ccbea7902a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dictput(d, l, item):\n",
    "    \"\"\"\n",
    "    Put item in nested dict using a list of keys\n",
    "\n",
    "    d: nested dict\n",
    "    l: list of keys\n",
    "    item: item to be put\n",
    "    \"\"\"\n",
    "    while l[0] in d and len(l)>1:  # transerse existing keys\n",
    "        d = d[l[0]]\n",
    "        l = l[1:]\n",
    "    while len(l)>1:  # add new keys as needed\n",
    "        d[l[0]] = dict()\n",
    "        d = d[l[0]]\n",
    "        l = l[1:]\n",
    "    d[l[0]] = item\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12cd0172-31ad-482e-b18c-6306c22ef761",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convenience functions\n",
    "def dget(l):\n",
    "    return dictget(data, l)\n",
    "def dknown(l):\n",
    "    return dictknown(data, l)\n",
    "def dput(l, item):\n",
    "    return dictput(data, l, item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7eb0f6f-761e-46eb-acf9-bd9634fec0c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def showdata():\n",
    "    \"\"\"\n",
    "    Display structure of data\n",
    "    \"\"\"\n",
    "    for k, d in data.items():\n",
    "        print(k)\n",
    "        for k2, d2 in d.items():\n",
    "            print('  ', k2)\n",
    "            try:\n",
    "                for k3, d3 in d2.items():\n",
    "                    print('    ', k3)\n",
    "                    try:\n",
    "                        for k4, d4 in d3.items():\n",
    "                            print('      ', k4)\n",
    "                            try:\n",
    "                                for k5, d5 in d4.items():\n",
    "                                    print('        ', k5)\n",
    "                                    try:\n",
    "                                        for k6, d6 in d5.items():\n",
    "                                            print('          ', k6)\n",
    "                                    except:\n",
    "                                        pass\n",
    "                            except:\n",
    "                                pass\n",
    "                    except:\n",
    "                        pass\n",
    "            except:\n",
    "                pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dad45d9-b145-4367-a91c-6f5c8cf10f5e",
   "metadata": {},
   "source": [
    "## Set experiments, regions, date ranges, variables, frequencies etc\n",
    "1deg_jra55_iaf_omip2_cycle6\n",
    "\n",
    "1deg_jra55_iaf_omip2_cycle6_jra55v150_extension\n",
    "\n",
    "025deg_jra55_iaf_omip2_cycle6\n",
    "\n",
    "025deg_jra55_iaf_omip2_cycle6_jra55v150_extension\n",
    "\n",
    "01deg_jra55v140_iaf_cycle4\n",
    "\n",
    "01deg_jra55v140_iaf_cycle4_jra55v150_extension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb94c24b-6b81-43cd-8831-9410a69b7860",
   "metadata": {},
   "outputs": [],
   "source": [
    "addexpt('1', {'model':'access-om2',\n",
    "              'expts': ['1deg_jra55_iaf_omip2_cycle6',\n",
    "                        '1deg_jra55_iaf_omip2_cycle6_jra55v150_extension'],\n",
    "              'gridfix': '/g/data/ik11/grids/ocean_grid_10.nc',\n",
    "            # 'gridpaths': ['/g/data/ik11/grids/ocean_grid_10.nc']\n",
    "              'gridpaths': []\n",
    "             })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4325ca3a-f1bb-4bca-a47f-9ae1571646ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "addexpt('025', {'model':'access-om2-025',\n",
    "                'expts': ['025deg_jra55_iaf_omip2_cycle6',\n",
    "                          '025deg_jra55_iaf_omip2_cycle6_jra55v150_extension'],\n",
    "                'gridfix': '/g/data/ik11/grids/ocean_grid_025.nc',\n",
    "                # 'gridpaths': ['/g/data/ik11/grids/ocean_grid_025.nc']\n",
    "                'gridpaths': []\n",
    "               })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "304ffe86-1feb-411c-85ce-484489e92881",
   "metadata": {},
   "outputs": [],
   "source": [
    "addexpt('01', {'model':'access-om2-01',\n",
    "               'expts': ['01deg_jra55v140_iaf_cycle4',\n",
    "                         '01deg_jra55v140_iaf_cycle4_jra55v150_extension'],\n",
    "               'gridfix': '/g/data/ik11/grids/ocean_grid_01.nc', \n",
    "               'gridpaths': [#'/g/data/ik11/grids/ocean_grid_01.nc', \n",
    "                             '/g/data/cj50/access-om2/raw-output/access-om2-01/01deg_jra55v140_iaf/output000/ocean/ocean-2d-area_t.nc',\n",
    "                             '/g/data/cj50/access-om2/raw-output/access-om2-01/01deg_jra55v140_iaf/output000/ocean/ocean-2d-area_u.nc']\n",
    "              })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f763ed5-7ec4-41c1-9ef6-130671e5636d",
   "metadata": {},
   "outputs": [],
   "source": [
    "showdata()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37986e2c-908b-42d8-8992-f5f30fe31597",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set date range\n",
    "\n",
    "tstart = pd.to_datetime('1958', format='%Y')\n",
    "# tend = pd.to_datetime('2023-01-01', format='%Y-%m-%d')\n",
    "tend = pd.to_datetime('2023', format='%Y')\n",
    "# tend = tstart + pd.DateOffset(years=30)\n",
    "timerange = slice(tstart, tend)\n",
    "firstyear = pd.to_datetime(tstart).year  # assumes tstart is 1 January!\n",
    "lastyear = pd.to_datetime(tend).year-1  # assumes tend is 1 January!\n",
    "yearrange = str(firstyear)+'-'+str(lastyear)\n",
    "print('yearrange =', yearrange, 'complete years')\n",
    "print('tstart =', tstart)\n",
    "print('tend =', tend)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59fadf5e-1ab1-410b-9d4c-d3ec9aaf84de",
   "metadata": {},
   "outputs": [],
   "source": [
    "varnames = [\n",
    "            'u', 'v',\n",
    "            'pot_temp',\n",
    "            'salt',\n",
    "            'pot_rho_0', 'pot_rho_2',\n",
    "            'sea_level',\n",
    "            'net_sfc_heating', 'frazil_3d_int_z',  # heat: https://forum.access-hive.org.au/t/net-surface-heat-and-freshwater-flux-variables/993/2\n",
    "            'pme_river',  # water\n",
    "            'sfc_salt_flux_ice', 'sfc_salt_flux_restore',  # salt\n",
    "            # 'mh_flux',  # sea ice melt\n",
    "            # 'sfc_hflux_coupler',\n",
    "            # 'sfc_hflux_from_runoff',\n",
    "            # 'sfc_hflux_pme',\n",
    "            # 'net_sfc_heating', 'frazil_3d_int_z',  # Net surface heat flux into ocean is net_sfc_heating + frazil_3d_int_z: https://github.com/COSIMA/access-om2/issues/139#issuecomment-639278547\n",
    "            # 'swflx',\n",
    "            # 'lw_heat',\n",
    "            # 'sens_heat',\n",
    "            # 'evap_heat',\n",
    "            # 'fprec_melt_heat',\n",
    "           ]"
   ]
  },
  {
   "cell_type": "raw",
   "id": "469e9b40-73ed-4b55-ad0a-2588213f1e5b",
   "metadata": {},
   "source": [
    "varnames = [\n",
    "            'u', 'v',\n",
    "            'pot_temp',\n",
    "            'sea_level'\n",
    "           ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ab81aa8-7a58-4a7f-8b87-28ddeb783598",
   "metadata": {},
   "outputs": [],
   "source": [
    "varnames = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83688eb4-6a7c-49ad-82f6-9a53e7db8f0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "varnames = [\n",
    "            'aice_m'\n",
    "            ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67de7890-d3f2-4a2c-9653-0cf1d1124959",
   "metadata": {},
   "outputs": [],
   "source": [
    "tgriddims = ['xt_ocean', 'yt_ocean']\n",
    "ugriddims = ['xu_ocean', 'yu_ocean']\n",
    "tgridvars = ['geolon_t', 'geolat_t', 'area_t', 'dxt', 'dyt', 'ht']\n",
    "ugridvars = ['geolon_c', 'geolat_c', 'area_u', 'dxu', 'dyu', 'hu']\n",
    "gridvars = tgridvars+ugridvars+tgriddims+ugriddims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d585023e-11fe-4aa1-b7f6-42e546aad905",
   "metadata": {},
   "outputs": [],
   "source": [
    "frequencies = ['1 monthly']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "333b0b1d-71ec-4641-ab75-2e1d3fb27ae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for the North Atlantic: 70W-0E, 40N-70N\n",
    "regions = OrderedDict([\n",
    "    ('NA', {'lon': slice(-70, 0), 'lat': slice(40, 70)}),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25eb8eba-4261-4cda-a1a0-cbb2514107ad",
   "metadata": {},
   "source": [
    "## Calculations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5761c8a9-58c2-4d1d-965a-7328854ff308",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "788708e6-0c39-429e-8626-0e4b84448964",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadalldata(data, regions, freqs, varnames, gridvars, timerange=timerange, ncfiles=None):\n",
    "    region = 'global'\n",
    "    reduction = 'unreduced'\n",
    "\n",
    "    varnames = list(set(varnames))\n",
    "\n",
    "    if not isinstance(ncfiles, list):\n",
    "        ncfiles = [ncfiles]*len(varnames)  # use the same ncfile for all variables\n",
    "\n",
    "    for expt in data.keys():\n",
    "        print(expt)\n",
    "        for freq in freqs:\n",
    "            kkey = [expt, region, freq, reduction]\n",
    "            for varname, ncfile in zip(varnames, ncfiles):\n",
    "                if not dknown(kkey+[varname]):\n",
    "                    if ncfile is None:\n",
    "                        print('loading', varname)\n",
    "                    else:\n",
    "                        print('loading', varname, 'from', ncfile)\n",
    "                    ds1 = cc.querying.getvar(dget([expt, 'expts'])[0], varname, session, frequency=freq, ncfile=ncfile, decode_coords=False, start_time=str(timerange.start))\n",
    "                    ds2 = cc.querying.getvar(dget([expt, 'expts'])[1], varname, session, frequency=freq, ncfile=ncfile, decode_coords=False, end_time=str(timerange.stop))\n",
    "                    dscombined = xr.concat([ds1, ds2], 'time').sel(time=timerange)\n",
    "                    dscombined.attrs['ncfiles'] = ds1.attrs['ncfiles'] + ds2.attrs['ncfiles']\n",
    "                    dscombined.attrs['description'] = [ds1.attrs['description'], ds2.attrs['description']]\n",
    "                    try:\n",
    "                        dscombined.attrs['notes'] = [ds1.attrs['notes'], ds2.attrs['notes']]\n",
    "                    except KeyError:\n",
    "                        pass\n",
    "                    dput(kkey+[varname], dscombined)\n",
    "\n",
    "        freq = 'static'\n",
    "\n",
    "        # first look for grid data in gridpaths (to avoid NaNs)\n",
    "        grids = [(p, xr.open_dataset(p, chunks='auto')) for p in dget([expt, 'gridpaths'])]\n",
    "        for k in gridvars:\n",
    "            kkey = [expt, region, freq, k]\n",
    "            if not dknown(kkey):\n",
    "                for (p, g) in grids:\n",
    "                    try:\n",
    "                        dput(kkey, g[k])\n",
    "                        da = g[k]\n",
    "                        print(k, 'loaded from', p)\n",
    "                        break\n",
    "                    except:\n",
    "                        continue\n",
    "                try:\n",
    "                    da = da.rename({'grid_x_T': 'xt_ocean', 'grid_y_T': 'yt_ocean'}) # fix for 01deg\n",
    "                    da.coords['xt_ocean'] = dget(kkey[0:-1]+['xt_ocean']).values\n",
    "                    da.coords['yt_ocean'] = dget(kkey[0:-1]+['yt_ocean']).values\n",
    "                    dput(kkey, da)\n",
    "                except:\n",
    "                    pass\n",
    "                try:\n",
    "                    da = da.rename({'grid_x_C': 'xu_ocean', 'grid_y_C': 'yu_ocean'}) # fix for 01deg\n",
    "                    da.coords['xu_ocean'] = dget(kkey[0:-1]+['xu_ocean']).values\n",
    "                    da.coords['yu_ocean'] = dget(kkey[0:-1]+['yu_ocean']).values\n",
    "                    dput(kkey, da)\n",
    "                except:\n",
    "                    pass\n",
    "\n",
    "        # then try to find any missing grid data via cookbook database (will have NaNs for land tiles)\n",
    "        for k in gridvars:\n",
    "            kkey = [expt, region, freq, k]\n",
    "            if not dknown(kkey):\n",
    "                try:\n",
    "                    da = cc.querying.getvar(dget([expt, 'expts'])[0], k, session, frequency='static', decode_coords=False, n=1)\n",
    "                    dput(kkey, da)\n",
    "                    print(k, 'loaded from', da.attrs['ncfiles'][0])\n",
    "                except:\n",
    "                    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5c417c7-cfa6-4a2e-b31c-4e445dc0fda1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "loadalldata(data, regions, frequencies, varnames, gridvars, timerange=timerange, ncfiles=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3c7d961-76ca-4be7-91db-296c119dcb64",
   "metadata": {},
   "outputs": [],
   "source": [
    "showdata()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d8f0b62-f5c1-49fa-bccb-e406353444da",
   "metadata": {},
   "source": [
    "## Fix up processor masking in coords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60e421dc-83b9-4e11-b60d-e6c2cff458c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "freq = 'static'\n",
    "for expt in data.keys():\n",
    "    print(expt)\n",
    "    for varname in ['geolon_t', 'geolat_t', 'geolon_c', 'geolat_c']:\n",
    "        print(expt, varname)\n",
    "        dget([expt,'global',freq,varname]).data = xr.open_dataset(dget([expt])['gridfix'])[varname].data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b99c52c1-51b9-419b-b4e3-4d9fc2501239",
   "metadata": {},
   "source": [
    "## Calculating dzt, dzu\n",
    "\n",
    "These are the same (equal to diff of `st_edges_ocean`) except in the bottom partial cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0e6f5b6-e2a8-49f0-8598-a90d3e8bad78",
   "metadata": {},
   "outputs": [],
   "source": [
    "basedir = '/g/data/v45/aek156/notebooks/github/aekiss/OSNAP/data-nco-new3/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc11cfd2-d337-4b12-94a5-7a3b8be68f1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "freq = 'static'\n",
    "for expt in data.keys():\n",
    "    print(expt)\n",
    "    for varname in ['dzt', 'dzu']:\n",
    "        for region, region_data in regions.items():\n",
    "            gvardir = os.path.join(basedir, 'global', freq, expt).replace(' ', '_')\n",
    "            gvarf = os.path.join(gvardir, '-'.join(['access-om2', expt, varname])+'.nc')\n",
    "            os.makedirs(gvardir, exist_ok=True)\n",
    "\n",
    "            vardir = os.path.join(basedir, region, freq, expt).replace(' ', '_')\n",
    "            varf = os.path.join(vardir, '-'.join(['access-om2', expt, varname])+'.nc')\n",
    "            os.makedirs(vardir, exist_ok=True)\n",
    "            \n",
    "            st_ocean = dget([expt,'global','1 monthly','unreduced','salt']).st_ocean\n",
    "            st_edges_ocean = cc.querying.getvar(dget([expt, 'expts'])[0], 'st_edges_ocean', session, frequency='1 monthly', n=1)\n",
    "            \n",
    "            dzt_1d = st_edges_ocean.diff('st_edges_ocean').assign_coords(st_edges_ocean=st_ocean.data).rename({'st_edges_ocean': 'st_ocean'})\n",
    "\n",
    "            if varname == 'dzt':\n",
    "                longname = 'tcell thickness'\n",
    "                dzt = 0*dget([expt,'global','1 monthly','unreduced','salt']).isel(time=0) + dzt_1d\n",
    "                \n",
    "                kmt = cc.querying.getvar(dget([expt, 'expts'])[0], 'kmt', session, frequency='static', n=1).round().astype('int64')\n",
    "                kmt = xr.where(kmt<0, 1, kmt).compute() - 1  # -1 to use zero-based indexing\n",
    "    \n",
    "                ht = dget([expt,'global','static','ht'])\n",
    "                dzt_bottom = ht - st_edges_ocean[kmt]\n",
    "    \n",
    "            elif varname == 'dzu':  # (confusingly) keep same variable names as for t cells\n",
    "                longname = 'ucell thickness'\n",
    "                dzt = 0*dget([expt,'global','1 monthly','unreduced','u']).isel(time=0) + dzt_1d\n",
    "                \n",
    "                kmt = cc.querying.getvar(dget([expt, 'expts'])[0], 'kmu', session, frequency='static', n=1).round().astype('int64')\n",
    "                kmt = xr.where(kmt<0, 1, kmt).compute() - 1  # -1 to use zero-based indexing\n",
    "    \n",
    "                ht = dget([expt,'global','static','hu'])\n",
    "                dzt_bottom = ht - st_edges_ocean[kmt]\n",
    "            else:\n",
    "                raise ValueError('Bad varname: '+varname)\n",
    "\n",
    "            dzt_fixed = xr.where(dzt.st_ocean!=dzt.st_ocean[kmt],\n",
    "                     dzt,\n",
    "                     dzt_bottom, # put partial cell height in bottom cells\n",
    "                     keep_attrs=True).rename(varname)\n",
    "            dzt_fixed = dzt_fixed.assign_attrs({'long_name': longname, 'units':'m'} | {k:ht.attrs[k] for k in ['contact', 'email', 'description', 'notes']})\n",
    "\n",
    "            for k in tgridvars+ugridvars+['st_edges_ocean', 'time']:  # drop extraneous coords\n",
    "                try:\n",
    "                    dzt_fixed = dzt_fixed.drop_vars(k)\n",
    "                except ValueError:\n",
    "                    pass\n",
    "            \n",
    "            gds = dzt_fixed\n",
    "\n",
    "# output global grid data\n",
    "            if False: #os.path.exists(gvarf):\n",
    "                print('--- skipped existing', gvarf)\n",
    "                pass\n",
    "            else:\n",
    "                print('saving', gvarf)\n",
    "                # gds.to_netcdf(gvarf+'-PARTIAL', encoding={\"zlib\": True, \"complevel\": 9} )\n",
    "                gds.to_netcdf(gvarf+'-PARTIAL')\n",
    "                os.rename(gvarf+'-PARTIAL', gvarf)\n",
    "\n",
    "# extract spatial regions\n",
    "            lons = ','.join([gds.dims[-1],\n",
    "                                   repr(float(region_data['lon'].start)),\n",
    "                                   repr(float(region_data['lon'].stop))])\n",
    "            lats = ','.join([gds.dims[-2],\n",
    "                                   repr(float(region_data['lat'].start)),\n",
    "                                   repr(float(region_data['lat'].stop))])\n",
    "            if False: #os.path.exists(varf):\n",
    "                print('--- skipped existing', varf)\n",
    "                pass\n",
    "            else:\n",
    "                cmd = ['ncks', '--deflate', '9', '-v', varname, '-d', lons, '-d', lats, '-O', gvarf, varf]\n",
    "                print(' '.join(cmd))\n",
    "                proc = subprocess.run(cmd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dfd3b3c-a995-4daf-9afb-c35d6873feeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "dzt_fixed.isel(yu_ocean=500).sel(xu_ocean=slice(-200,-100)).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f447b97-d33d-43ed-aafa-a5414e74431b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dzt_fixed.isel(st_ocean=kmt-1).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03a13639-1aaa-4644-8536-33ae3dc82ef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "dzt_fixed.isel(st_ocean=kmt).plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4639512-6480-437b-8b75-7fcbe18bda67",
   "metadata": {},
   "source": [
    "## Get filenames to use with nco\n",
    "\n",
    "#### Regional subsetting:\n",
    "- https://nco.sourceforge.net/nco.html#ncks-netCDF-Kitchen-Sink\n",
    "- https://nco.sourceforge.net/nco.html#Hyperslabs-1 - use reals for coord values, integers for indices\n",
    "```\n",
    "ncks -d xu_ocean,-70.0,0.0 -d yu_ocean,40.0,70.0 in.nc out.nc\n",
    "```\n",
    "takes ~7sec per 1-month file at 0.1°\n",
    "\n",
    "Get in.nc from `dget(['01','global','1 monthly','unreduced','u']).attrs['ncfiles']`\n",
    "\n",
    "#### Concatenation in time:\n",
    "- https://nco.sourceforge.net/nco.html#Concatenation\n",
    "- https://nco.sourceforge.net/nco.html#ncrcat-netCDF-Record-Concatenator\n",
    "```\n",
    "ncrcat in1.nc in2.nc ... inN.nc out.nc\n",
    "```\n",
    "NB: inputs must be in chronological order!\n",
    "\n",
    "#### Trim in time:\n",
    "- https://nco.sourceforge.net/nco.html#Hyperslabs\n",
    "\n",
    "`ncfiles` list may contain more files than we need, so trim in time with `ncks`. Negative integer indices are offsets from the end, e.g. `-d time,,-2` drops the final time slice.\n",
    "\n",
    "- `for f in access-om2-1* ; do echo $f; ncdump -t -v time $f | grep 202[34]-; done` shows the 1deg data is already OK\n",
    "\n",
    "- `for f in access-om2-025* ; do echo $f; ncdump -t -v time $f | grep 202[34]-; done` shows 4 extra time slices in 0.25deg, so use `ncks -d time,,-5 in.nc out.nc` to trim them\n",
    "```\n",
    "    \"2023-01-14 12\", \"2023-02-13\", \"2023-03-14 12\", \"2023-04-14\" ;\n",
    "```\n",
    "\n",
    "- `for f in access-om2-01* ; do echo $f; ncdump -t -v time $f | grep 202[34]-; done` shows 1 extra time slice in 0.1deg, so use `ncks -d time,,-2 in.nc out.nc` to trim them\n",
    "```\n",
    "    \"2023-01-16 12\" ;\n",
    "```"
   ]
  },
  {
   "cell_type": "raw",
   "id": "07824c14-4c92-44a0-b62b-1ff305e43bf3",
   "metadata": {},
   "source": [
    "basedir = '/g/data/v45/aek156/notebooks/github/aekiss/OSNAP/data-nco/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "786214d8-4e62-42f4-8419-637ea705cb47",
   "metadata": {},
   "outputs": [],
   "source": [
    "vardata.dims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0cde536-0726-463a-85c1-861f03598e02",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# extract and concatenate data for each region using nco\n",
    "reduction = 'unreduced'\n",
    "for expt in data.keys():\n",
    "    for region, region_data in regions.items():\n",
    "        for freq in frequencies:\n",
    "            kkey = [expt, region, freq, reduction]\n",
    "            for varname, vardata in dget([expt, 'global', freq, reduction]).items():\n",
    "                vardir = os.path.join(basedir, region, freq, expt, varname).replace(' ', '_')\n",
    "                os.makedirs(vardir, exist_ok=True)\n",
    "                \n",
    "                # extract spatial regions\n",
    "                cice = '/ice/OUTPUT/' in vardata.attrs['ncfiles'][0] #data['1']['global']['1 monthly']['unreduced']['aice_m'].attrs['ncfiles'][0]\n",
    "                if cice:\n",
    "                    # cice output don't contain coord data, so need to slice by index, not coord value\n",
    "# BUG: fails for data not on t-points\n",
    "                    coord = dget([expt, 'global', 'static', 'xt_ocean'])\n",
    "                    lonsarray = coord.sel(xt_ocean=region_data['lon']).values\n",
    "                    lons = ','.join([vardata.dims[-1],\n",
    "                                           repr(list(coord).index(lonsarray[0])),\n",
    "                                           repr(list(coord).index(lonsarray[-1]))])\n",
    "                    coord = dget([expt, 'global', 'static', 'yt_ocean'])\n",
    "                    latsarray = coord.sel(yt_ocean=region_data['lat']).values\n",
    "                    lats = ','.join([vardata.dims[-2],\n",
    "                                           repr(list(coord).index(latsarray[0])),\n",
    "                                           repr(list(coord).index(latsarray[-1]))])\n",
    "                else:\n",
    "                    lons = ','.join([vardata.dims[-1],\n",
    "                                           repr(float(region_data['lon'].start)),\n",
    "                                           repr(float(region_data['lon'].stop))])\n",
    "                    lats = ','.join([vardata.dims[-2],\n",
    "                                           repr(float(region_data['lat'].start)),\n",
    "                                           repr(float(region_data['lat'].stop))])\n",
    "                for count, nc in enumerate(vardata.attrs['ncfiles']):\n",
    "                    if expt == '01':\n",
    "                        outf = os.path.join(vardir, os.path.basename(nc))\n",
    "                    else:\n",
    "                        outf = os.path.join(vardir, '-'.join([varname, f'{count:04}']))\n",
    "                    if os.path.exists(outf):\n",
    "                        # print('--- skipped existing', outf)\n",
    "                        pass\n",
    "                    else:\n",
    "                        cmd = ['ncks', '-v', varname, '-d', lons, '-d', lats, nc, outf]\n",
    "                        # print(' '.join(cmd))\n",
    "                        proc = subprocess.run(cmd)\n",
    "    #                     break\n",
    "                \n",
    "                # concatenate in time\n",
    "                outf = os.path.join(basedir, '-'.join(['access-om2', expt, varname])+'.nc')\n",
    "                if os.path.exists(outf):\n",
    "                    print('--- skipped existing', outf)\n",
    "                else:\n",
    "                    print(outf)\n",
    "                    infiles = sorted(glob.glob(os.path.join(vardir, '*'))) #.sort()\n",
    "                    cmd = ['ncrcat', *infiles, outf]\n",
    "                    # print(' '.join(cmd))\n",
    "                    proc = subprocess.run(cmd)\n",
    "                    # break\n",
    "                # break\n",
    "\n",
    "                if cice:\n",
    "# shift time back by 1 day if cice output https://github.com/COSIMA/cosima-recipes/blob/main/Recipes/Sea_Ice_Coordinates.ipynb\n",
    "                    cmd = ['ncatted', '-a' \"units,time,o,c,days since 1957-12-31\", outf]\n",
    "                    print(' '.join(cmd))\n",
    "                    proc = subprocess.run(cmd)\n",
    "\n",
    "                # trim in time\n",
    "                tvardir = os.path.join(basedir, 'trimmed', '-'.join(['access-om2', expt]))\n",
    "                os.makedirs(tvardir, exist_ok=True)\n",
    "                toutf = os.path.join(tvardir, os.path.basename(outf))\n",
    "                \n",
    "                if os.path.exists(toutf):\n",
    "                    print('--- skipped existing', toutf)\n",
    "                else:\n",
    "                    if expt == '01':\n",
    "                        idx = -2\n",
    "                    elif expt == '025':\n",
    "                        if cice:\n",
    "                            idx = -2\n",
    "                        else:\n",
    "                            idx = -5\n",
    "                    else:\n",
    "                        idx = None\n",
    "    \n",
    "                    if idx:\n",
    "                        cmd = ['ncks', '-d', 'time,,'+str(idx), outf, toutf]\n",
    "                        print(' '.join(cmd))\n",
    "                        proc = subprocess.run(cmd)\n",
    "                    else:\n",
    "                        print('shutil.copy2({}, {})'.format(outf, toutf))\n",
    "                        shutil.copy2(outf, toutf)\n",
    "                    # break\n",
    "    # break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25b6c0e5-2e99-44b6-95ed-d4fbacf05b8e",
   "metadata": {},
   "source": [
    "now check with\n",
    "```\n",
    "for f in trimmed/*/* ; do echo $f; ncdump -t -v time $f | grep 202[2-4]-; done\n",
    "```\n",
    "and copy to globus with\n",
    "```\n",
    "rsync -vrltoD data-nco/trimmed/* /g/data/ik11/globus/osnap-access-om2\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c89baaf-2e4e-44b6-bf6d-b78ed1b3c273",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "2c732186-1e5d-4867-898a-0a43cc5d2c35",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "# output global grid data\n",
    "region = 'global'\n",
    "freq = 'static'\n",
    "for expt in data.keys():\n",
    "    print(expt)\n",
    "    vardir = os.path.join(basedir, region, freq, expt).replace(' ', '_')\n",
    "    os.makedirs(vardir, exist_ok=True)\n",
    "    for varname in gridvars:\n",
    "        print(varname)\n",
    "# output global grid data\n",
    "        outf = os.path.join(vardir, '-'.join(['access-om2', expt, varname])+'.nc')\n",
    "        # print(outf)\n",
    "        if os.path.exists(outf):\n",
    "            print('--- skipped existing', outf)\n",
    "        else:\n",
    "            print('saving', outf)\n",
    "            dget([expt, region, freq, varname]).to_netcdf(outf+'-PARTIAL')\n",
    "            os.rename(outf+'-PARTIAL', outf)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f090c22a-5787-4b65-9772-c31be65a2756",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "freq = 'static'\n",
    "for expt in data.keys():\n",
    "    for varname in tgridvars+ugridvars:\n",
    "        for region, region_data in regions.items():\n",
    "            gvardir = os.path.join(basedir, 'global', freq, expt).replace(' ', '_')\n",
    "            gvarf = os.path.join(gvardir, '-'.join(['access-om2', expt, varname])+'.nc')\n",
    "            os.makedirs(gvardir, exist_ok=True)\n",
    "        \n",
    "            vardir = os.path.join(basedir, region, freq, expt).replace(' ', '_')\n",
    "            varf = os.path.join(vardir, '-'.join(['access-om2', expt, varname])+'.nc')\n",
    "            os.makedirs(vardir, exist_ok=True)\n",
    "\n",
    "            gds = dget([expt, 'global', freq, varname])\n",
    "\n",
    "# output global grid data\n",
    "            if os.path.exists(gvarf):\n",
    "                print('--- skipped existing', gvarf)\n",
    "                pass\n",
    "            else:\n",
    "                print('saving', gvarf)\n",
    "                for k in tgridvars+ugridvars:  # drop extraneous coords\n",
    "                    try:\n",
    "                        gds = gds.drop_vars(k)\n",
    "                    except ValueError:\n",
    "                        pass\n",
    "                gds.to_netcdf(gvarf+'-PARTIAL')\n",
    "                os.rename(gvarf+'-PARTIAL', gvarf)\n",
    "\n",
    "# extract spatial regions\n",
    "            lons = ','.join([gds.dims[-1],\n",
    "                                   repr(float(region_data['lon'].start)),\n",
    "                                   repr(float(region_data['lon'].stop))])\n",
    "            lats = ','.join([gds.dims[-2],\n",
    "                                   repr(float(region_data['lat'].start)),\n",
    "                                   repr(float(region_data['lat'].stop))])\n",
    "            if os.path.exists(varf):\n",
    "                print('--- skipped existing', varf)\n",
    "                pass\n",
    "            else:\n",
    "                cmd = ['ncks', '-v', varname, '-d', lons, '-d', lats, gvarf, varf]\n",
    "                print(' '.join(cmd))\n",
    "                proc = subprocess.run(cmd)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b38eab04-9ae5-4e9b-9be5-fc4b640df715",
   "metadata": {},
   "source": [
    "copy to globus with\n",
    "```\n",
    "mkdir -p /g/data/ik11/globus/osnap-access-om2/access-om2-1/grid\n",
    "mkdir -p /g/data/ik11/globus/osnap-access-om2/access-om2-025/grid\n",
    "mkdir -p /g/data/ik11/globus/osnap-access-om2/access-om2-01/grid\n",
    "```\n",
    "```\n",
    "rsync -vrltoD data-nco/NA/static/1/* /g/data/ik11/globus/osnap-access-om2/access-om2-1/grid\n",
    "rsync -vrltoD data-nco/NA/static/025/* /g/data/ik11/globus/osnap-access-om2/access-om2-025/grid\n",
    "rsync -vrltoD data-nco/NA/static/01/* /g/data/ik11/globus/osnap-access-om2/access-om2-01/grid\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a8e331c-9a83-40a4-8acf-e4ea6b0362c5",
   "metadata": {},
   "source": [
    "copy to globus with\n",
    "```\n",
    "mkdir -p /g/data/ik11/globus/osnap-access-om2/access-om2-1/grid\n",
    "mkdir -p /g/data/ik11/globus/osnap-access-om2/access-om2-025/grid\n",
    "mkdir -p /g/data/ik11/globus/osnap-access-om2/access-om2-01/grid\n",
    "```\n",
    "```\n",
    "rsync -vrltoD data-nco-new/NA/static/1/* /g/data/ik11/globus/osnap-access-om2/access-om2-1/grid\n",
    "rsync -vrltoD data-nco-new/NA/static/025/* /g/data/ik11/globus/osnap-access-om2/access-om2-025/grid\n",
    "rsync -vrltoD data-nco-new/NA/static/01/* /g/data/ik11/globus/osnap-access-om2/access-om2-01/grid\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1efdb54b-1ec7-4429-a531-8211e9e40f61",
   "metadata": {},
   "source": [
    "## Check fixed-up coords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caf83cc4-ad36-4c00-9ef0-38f441e763a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "freq = 'static'\n",
    "for expt in data.keys():\n",
    "    print(expt)\n",
    "    for varname in ['geolon_t', 'geolat_t', 'geolon_c', 'geolat_c']:\n",
    "        old = xr.open_dataset(basedir+'../data-nco/NA/static/'+expt+'/access-om2-'+expt+'-'+varname+'.nc')[varname].load()\n",
    "        new = xr.open_dataset(basedir+'../data-nco-new2/NA/static/'+expt+'/access-om2-'+expt+'-'+varname+'.nc')[varname].load()\n",
    "        diff = (old.data-new.data)\n",
    "        print(expt, varname, diff.min(), diff.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d3b7457-ecb6-4964-89ac-8c411de51d44",
   "metadata": {},
   "outputs": [],
   "source": [
    "old"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29ef672e-c918-4ff9-8b1d-2b8eb1cb3c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "409acf75-1dac-4d71-8efb-b0c1a463f085",
   "metadata": {},
   "outputs": [],
   "source": [
    "old.data.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6da21253-30ea-419b-af9c-f01d2ad3fe8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "new.data.dtype"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
